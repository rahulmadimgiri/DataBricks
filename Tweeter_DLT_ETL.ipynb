{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cfd7cf1-5ee3-4c80-a0dd-c98bf47144d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Real-Time Twitter Data Ingestion Using Delta Live Tables (DLT) Streaming Pipeline\n",
    "\n",
    "###Flow Diagram Of Project\n",
    "\n",
    "![](/Volumes/data_gov/data_gov_test/image/tweitter_fow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7070eccb-a370-4291-b82a-26604ce402ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Integrate with the Twitter API to retrieve data and ingest it into DBFS (Databricks File System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4070ada7-0cbc-47ed-a45d-a1cbc28776c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install the lib tweepy"
    }
   },
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4deb8be5-47e4-4b4a-8633-406fccc0ec1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc787db7-d831-4351-9b85-2f55b5961710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extracting the Twwiter Time line data from Own Acccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da762e6d-339e-4bd5-92b6-673bb3df3b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAOnN2QEAAAAAkDVTjCBFdlU%2BLAravXrKB%2FkpuSY%3DYSwPr6eCqKVoWDHPJcG2ZODNcPLRYfSSvi9K6jaEkpCOAuyPnC\"  \n",
    "USERNAME = 'madimgiri'       \n",
    "DBFS_PATH = '/Volumes/data_gov/data_gov_test/twitter/my_tweets.json'\n",
    "\n",
    "# === Twitter API Headers ===\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {BEARER_TOKEN}',\n",
    "    'User-Agent': 'v2UserTimelinePython'\n",
    "}\n",
    "\n",
    "# === Get user ID from username ===\n",
    "def get_user_id(username):\n",
    "    url = f'https://api.twitter.com/2/users/by/username/{username}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to get user ID: {response.status_code} {response.text}\")\n",
    "    return response.json()['data']['id']\n",
    "\n",
    "# === Retry wrapper with backoff ===\n",
    "def safe_request(url, headers, params, retries=3, wait=60):\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"⚠️ Rate limited. Waiting {wait} seconds (attempt {attempt + 1}/{retries})...\")\n",
    "            time.sleep(wait)\n",
    "        else:\n",
    "            raise Exception(f\"HTTP Error {response.status_code}: {response.text}\")\n",
    "    raise Exception(\"❌ Failed after retries due to rate limiting.\")\n",
    "\n",
    "# === Fetch up to 800 tweets (max 8 pages) ===\n",
    "def get_user_timeline(user_id, max_pages=8):\n",
    "    url = f'https://api.twitter.com/2/users/{user_id}/tweets'\n",
    "    params = {\n",
    "        'max_results': 100,\n",
    "        'tweet.fields': 'created_at,id,text'\n",
    "    }\n",
    "\n",
    "    all_tweets = []\n",
    "    next_token = None\n",
    "\n",
    "    for _ in range(max_pages):\n",
    "        if next_token:\n",
    "            params['pagination_token'] = next_token\n",
    "\n",
    "        response = safe_request(url, headers, params)\n",
    "        data = response.json()\n",
    "        tweets = data.get('data', [])\n",
    "        all_tweets.extend(tweets)\n",
    "\n",
    "        next_token = data.get('meta', {}).get('next_token')\n",
    "        if not next_token:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # Pause to respect Twitter API pacing\n",
    "\n",
    "    return all_tweets\n",
    "\n",
    "# === Save to local path (not FileStore, which is disabled) ===\n",
    "def save_to_local(tweets, file_path=DBFS_PATH):\n",
    "    local_path = f'/tmp{file_path}' if file_path.startswith('/dbfs') else file_path\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    with open(local_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tweets, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# === Main Script ===\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        user_id = get_user_id(USERNAME)\n",
    "        tweets = get_user_timeline(user_id)\n",
    "        save_to_local(tweets)\n",
    "        print(f\"✅ Saved {len(tweets)} tweets to local path: {DBFS_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6d9ab3-de00-4c86-91d5-bdbcf921d8b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read File from DBFS\n",
    "df = spark.read.format(\"json\").load(\"/Volumes/data_gov/data_gov_test/twitter/my_tweets.json\").show()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af023cfb-92f0-4e0c-9a89-fcf00bfdf34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/FileStore/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64343420-6c1c-482d-afe2-104910764236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4528630484871206,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tweeter_DLT_ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
